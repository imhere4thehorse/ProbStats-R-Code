---
title: "K-Fold Cross Validation"
author: "Kalie"
date: "2023-04-24"
output: html_document
---

```{r setup, include=FALSE}
#Diamonds
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)

#Linear Regression
library(caret)

#Ridge
library(glmnet)
```

## Cross Validation Overview

For cross validation, we have a training set and a testing set. We are training a model based on a set of data and testing that model to predict a new set of data based on the data we used to train the model. The result of testing the model (the testing set) will tell us how well our predictions and model are. We do this by comparing expected test errors. 

  Performance assessment: Seeing how well we can predict a future observation 
  Model Selection: We will want to choose between different fitting functions (model classes or a tuning parameter for a single method)
  
## K-Fold Cross Validation Overview

1) Randomly shuffle the dataset
2) Split the dataset into K groups
3) Choose a value for K
4) For each n iteration, take the nth group as a test set ()
5) Calculate the evaluation score (MSE/ meansquared error) 
6) Summarize effectiveness of model using the evaluation scores


We will be using the diamond dataset.
```{r}
diamonds
#Karat (independent), depth (independent) and price (dependent)
```

## K-Fold Cross Validation (Linear Regression)

```{r}
#same random variables produced each time
set.seed(1)
data(diamonds)

sample_size <- 1000
d <- diamonds[sample(nrow(diamonds), sample_size), ]

d$carat_count <- d$carat 
d$price_count <- d$price 
d$depth_count <- d$depth
#Created as a column in data set d

#specify the cross-validation method (in this case, k-fold)
train_control <- trainControl(method = "cv", number = 5)

#fit a regression model and use k-fold CV to evaluate performance
model <- train(price_count ~ carat_count + depth_count, data = d, method = "lm", trControl = train_control)

#k-fold CV               
print(model)

#view predictions for each fold
model$resample
```

## K-Fold Cross Validation (Ridge)

```{r}
set.seed(1)
data(diamonds)

sample_size <- 1000
d <- diamonds[sample(nrow(diamonds), sample_size), ]

d$carat_count <- d$carat 
d$price_count <- d$price
d$depth_count <- d$depth
#Created as a column in data set d

#specify the cross-validation method
train_control <- trainControl(method = "cv", number = 5)


#for ridge, alpha = 0
#set lambda to iterate from 0 to 100 times, by 1 each
model<- train(price_count ~ carat_count + depth_count,  data = d, method = "glmnet", lambda = seq(0, 100, by = 1), trControl = train_control, tuneGrid = expand.grid(alpha = 0, lambda = seq(0, 100, by = 1)))


print(model)

#view predictions for each fold
model$resample
#IMPORTANCE OF LAMBDA: A tuning parameter/penalty parameter that controls the strength of the penalty term in ridge regression and lasso regression. The penalty term refers to the amount of shrinkage, where data values are shrunk towards a central point, like the mean. Ridge and Lasso prevent overfitting through this penalty

#OPTIMAL LAMBDA: Optimal shrinkage for data to fit. The way to find optimal lambda is through cross-validation (best one is K-fold). No shrinkage applies when lambda is 0.
```

## K-Fold Cross Validation (Lasso)
```{r}
set.seed(1)
data(diamonds)

sample_size <- 1000
d <- diamonds[sample(nrow(diamonds), sample_size), ]

d$carat_count <- d$carat 
d$price_count <- d$price 
d$depth_count <- d$depth
#Created as a column in data set d


#for lasso, alpha = 1
#set lambda to iterate from 0 to 100 times, by 1 each
model<- train(price_count ~ carat_count + depth_count,  data = d, method = "glmnet", lambda = seq(0, 100, by = 1), trControl = train_control, tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 100, by = 1)))


print(model)


#view predictions for each fold
model$resample
```

